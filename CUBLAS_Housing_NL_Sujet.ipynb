{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUBLAS-Housing-NL-Sujet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichaelDasseville/Generating_music/blob/master/CUBLAS_Housing_NL_Sujet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6vkiFMytjep",
        "colab_type": "text"
      },
      "source": [
        "This exercise uses the housing data available by default on Colab. For a description of this data, see [here](https://developers.google.com/machine-learning/crash-course/california-housing-data-description). The example is losely based on Google's (excellent) course on [logistic regression](https://colab.research.google.com/notebooks/mlcc/logistic_regression.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=logisticregression-colab&hl=en#scrollTo=B5OwSrr1yIKD)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcD68GLZuXx_",
        "colab_type": "text"
      },
      "source": [
        "The data is stored in the CSV file `sample_data/california_housing_test.csv`. The target is in column\n",
        "`median_house_value`. The goal is to use the remaining columns to estimate whether the target is above 265000 (label `1`) or below (label `0`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT3pxDiHsEWR",
        "colab_type": "code",
        "outputId": "f4c388d0-7338-4a4d-8b18-8aac0f55612c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRRhIdNZv9OA",
        "colab_type": "code",
        "outputId": "72fc1464-6e3f-4ffd-8427-0ab2fbea817e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip install --upgrade git+git://github.com/frehseg/nvcc4jupyter.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/frehseg/nvcc4jupyter.git\n",
            "  Cloning git://github.com/frehseg/nvcc4jupyter.git to /tmp/pip-req-build-bfyewbv3\n",
            "  Running command git clone -q git://github.com/frehseg/nvcc4jupyter.git /tmp/pip-req-build-bfyewbv3\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.1-cp36-none-any.whl size=2095 sha256=9d6440e083693bb97d7f947c1ae825dd5e3a32e5a9c87a63cda1e339f6a32c93\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fpymo18y/wheels/a4/a5/24/17a2b61f9a725a10155cc6fca753aae28436921df21fa16114\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj-NjqUEsXa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbjoQnlen-g3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs0anw29rsBp",
        "colab_type": "code",
        "outputId": "35df442f-fa3d-4315-b76c-e8d0301032a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%cu \n",
        "/** CUBLAS example adapted from http://courses.cms.caltech.edu/cs179/ */\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <math.h>\n",
        "#include <fstream>\n",
        "\n",
        "/* Includes, cuda */\n",
        "#include <cuda_runtime.h>\n",
        "#include <cublas_v2.h>\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "/* Matrix size */\n",
        "#define data_columns  (9)\n",
        "#define print_rows (10)\n",
        "#define above_threshold (265000.0)\n",
        "\n",
        "/* transform matrix index to vector offset\n",
        "   Since CUDA uses column major, \n",
        "   ld = number of rows \n",
        "   Example of use: a[IDX2C(0, 1, 50)] */\n",
        "#define IDX2C(i,j,ld) (((j)*(ld))+(i))\n",
        "\n",
        "// Error handling\n",
        "// taken from https://stackoverflow.com/questions/14038589/what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess) \n",
        "   {\n",
        "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) exit(code);\n",
        "   }\n",
        "}\n",
        "\n",
        "/* Print an NxM array from the GPU */\n",
        "void print_GPU_array(float* A, int N, int M) {\n",
        "    // reserve memory on host\n",
        "    float* tmp = (float*) malloc(N*M*sizeof(float));\n",
        "    // copy A from device to host\n",
        "    gpuErrchk( cudaMemcpy(tmp, A, N*M * sizeof(float), cudaMemcpyDeviceToHost) );\n",
        "    //cublasGetVector(N*M, sizeof(float), A, 1, tmp, 1);\n",
        "    // print array\n",
        "    for (int i = 0;i<N;++i) {\n",
        "      for (int j = 0;j<M;++j) {\n",
        "        printf(\"%f \",tmp[IDX2C(i,j,N)]);\n",
        "      }\n",
        "      printf(\"\\n\");\n",
        "    }\n",
        "    free(tmp);\n",
        "}\n",
        "\n",
        "// generate random numbers in interval [min,max]\n",
        "float float_rand( float min, float max )\n",
        "{\n",
        "    float scale = rand() / (float) RAND_MAX; /* [0, 1.0] */\n",
        "    return min + scale * ( max - min );      /* [min, max] */\n",
        "}\n",
        "\n",
        "\n",
        "/* Read a csv file with a given number of rows and columns */\n",
        "void read_csv(const char* filename, float* data_array,size_t nbrow,size_t nbcol) {\n",
        "  string row_as_string;\n",
        "  string value;\n",
        "  double ioTemp;\n",
        "  ifstream infile;\n",
        "  infile.open(filename, ifstream::in);\n",
        "  size_t row_count = 0;\n",
        "\tif (infile.is_open())\n",
        "  {\n",
        "      // read the headers (and discard)\n",
        "\t\t\tgetline(infile, row_as_string, '\\n');\n",
        "      cout << \"headers: \" << row_as_string << \"!\" << std::endl;\n",
        "      for(int i = 0; i < nbrow; i++){\n",
        "  \t\t\tgetline(infile, row_as_string, '\\n');\n",
        "        // cout << \"read line \" << row_as_string << \"!\" << std::endl;\n",
        "\t\t\t\tistringstream line_stream(row_as_string);\n",
        "\t\t\t  for(int j = 0; j < nbcol; j++){\n",
        "          getline(line_stream, value, ',');\n",
        "\t\t\t\t\tioTemp = strtod(value.c_str(), NULL); \n",
        "          // cout << \"(\"<<i<<\",\"<<j<<\") = \"<< ioTemp << std::endl;\n",
        "\n",
        "\t\t\t\t\tdata_array[IDX2C(i,j,nbrow)] = ioTemp;\n",
        "\n",
        "\t\t\t\t}\n",
        "        ++row_count;\n",
        "\t\t\t}\n",
        "\t\tinfile.close();\n",
        "    cout << \"Read \" << row_count << \" rows.\" << std::endl;\n",
        "\t}\n",
        "\telse cout << \"Cannot open file.\" << endl;\n",
        "}\n",
        "\n",
        "/* Allocate memory and fill with given number of rows from housing file */\n",
        "void read_housing_csv(float** data_array, size_t nbrows) {\n",
        "    *data_array = (float *)malloc(nbrows * data_columns * sizeof(float));\n",
        "    \n",
        "    read_csv(\"sample_data/california_housing_test.csv\",*data_array,nbrows,data_columns);\n",
        "\n",
        "    cout << \"Data (first \"<<print_rows<<\" rows):\" << std::endl;\n",
        "    // Show some entries for double checking\n",
        "    for(int i = 0; i < nbrows && i < print_rows; i++){\n",
        "\t\t\tfor(int j = 0; j < data_columns; j++){\n",
        "\t\t\t\tcout << (*data_array)[IDX2C(i,j,nbrows)] << \"\\t\";\n",
        "\t\t\t}\n",
        "      cout << \"\\n\";\n",
        "\t\t}\n",
        "}\n",
        "\n",
        "/* Split data into inputs and labels. Allocated memory for inputs and labels.\n",
        "   Since cuBLAS is column major, each input is in a column.\n",
        "   We also add 1.0 as first element to each input vector.\n",
        "*/\n",
        "void get_inputs_and_labels(float* data_array, float** input_array, float** label_array, size_t nbrows, size_t nbcols, size_t* nb_inputs, size_t* nb_labels ) {\n",
        "    // The inputs are the first nbrows-1 columns.\n",
        "    // The labels are the last column (index nbrows-1), booleanized\n",
        "    // by the condition >= above_threshold\n",
        "    size_t nbcols_input = nbcols-1+1; // remove one column, add one for 1.0\n",
        "    size_t nbcols_labels = 2;\n",
        "    *nb_inputs = nbcols_input;\n",
        "    *nb_labels = nbcols_labels;\n",
        "    *input_array = (float *)malloc(nbrows * nbcols_input * sizeof(float));    \n",
        "    *label_array = (float *)malloc(nbrows * nbcols_labels * sizeof(float));    \n",
        "    //cout << &input_array << \" and \"<< &label_array << \" data \" << data_array << std::endl;\n",
        "    cout << \"Allocated memory: \" << nbrows << \" rows, \"<< nbcols_input << \" columns.\" << std::endl;\n",
        "\n",
        "    // Copy the data to X\n",
        "    for(int i = 0; i < nbrows; i++){\n",
        "      // Set the first element of each x to 1  \n",
        "      (*input_array)[IDX2C(0,i,nbcols_input)] = 1.0;\n",
        "      // Copy the rest of x\n",
        "\t\t\tfor(int j = 1; j < nbcols_input; j++){\n",
        "\t\t\t\t(*input_array)[IDX2C(j,i,nbcols_input)] = data_array[IDX2C(i,j-1,nbrows)];\n",
        "\t\t\t}\n",
        "      float median_house_value = data_array[IDX2C(i,nbcols-1,nbrows)];\n",
        "      (*label_array)[IDX2C(i,0,nbrows)] = 0.0;\n",
        "      (*label_array)[IDX2C(i,1,nbrows)] = 0.0;\n",
        "      if (median_house_value >= above_threshold) {\n",
        "        (*label_array)[IDX2C(i,0,nbrows)] = 1.0;\n",
        "      } else {\n",
        "        (*label_array)[IDX2C(i,1,nbrows)] = 1.0;        \n",
        "      }\n",
        "\t\t}    \n",
        "    \n",
        "    // Show some entries for double checking\n",
        "    cout << \"Inputs (first \"<<print_rows<<\"):\" << std::endl;\n",
        "\t  for(int j = 0; j < nbcols_input; j++){\n",
        "      for(int i = 0; i < nbrows && i < print_rows; i++){\n",
        "\t\t\t\tcout << (*input_array)[IDX2C(j,i,nbcols_input)] << \"\\t\";\n",
        "\t\t\t}\n",
        "      cout << \"\\n\";\n",
        "\t\t}\n",
        "    cout << \"Labels (first \"<<print_rows<<\"):\" << std::endl;\n",
        "    for(int i = 0; i < nbrows && i < print_rows; i++){\n",
        "\t\t\tfor(int j = 0; j < nbcols_labels; j++){\n",
        "\t\t\t\tcout << (*label_array)[IDX2C(i,j,nbrows)] << \"\\t\";\n",
        "\t\t\t}\n",
        "      cout << \"\\n\";\n",
        "\t\t}\n",
        "}\n",
        "\n",
        "/* Variables:\n",
        "  L : nb of layers\n",
        "  Ml : array of size of each layer\n",
        "  Wl : array of weights (matrix) for each layer\n",
        "  Bl : array of biases (vectors) for each layer\n",
        "  Xl : array of inputs (matrix)  for each layer; batch with one entry per column\n",
        "  Zl : array of activations (matrix)  for each layer; batch with one entry per column\n",
        "  Tl : array of type of each layer (0=softmax, 1=ReLU)\n",
        "*/\n",
        "\n",
        "/* Kernel to compute softmax on matrix of batch,\n",
        "   taking one thread for each element:\n",
        "      X = theta(Z)\n",
        "   Attention: \n",
        "   X has one row more than Z; the first row of X is identical \n",
        "   to 1 by convention!\n",
        "\n",
        "   Note: this kernel is not particularly efficient */\n",
        "__global__ \n",
        "void softmax_kernel(float* X, float* Z, int nb_lgn, int nb_col) {\n",
        "    const unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    const unsigned int col = i/nb_lgn;\n",
        "    const unsigned int lgn = i%nb_lgn;\n",
        "\n",
        "    if (col<nb_col && lgn<nb_lgn) {\n",
        "      // exp(z_k)/sum_t exp(z_t)\n",
        "      // First let x = exp(z)\n",
        "      X[IDX2C(lgn+1,col,nb_lgn+1)] = exp(Z[IDX2C(lgn,col,nb_lgn)]);\n",
        "      // normalize columns of x\n",
        "      // using only the first thread for each col\n",
        "      if (lgn == 0) {\n",
        "          // Assign first row of X to 1.0 by convention\n",
        "          X[IDX2C(0,col,nb_lgn+1)] = 1.0;\n",
        "          float s = 0.0;\n",
        "          for (int j = 0; j<nb_lgn; ++j) {\n",
        "             s += X[IDX2C(j+1,col,nb_lgn+1)];\n",
        "          }\n",
        "          // divide all elements\n",
        "          for (int j = 0; j<nb_lgn; ++j) {\n",
        "             X[IDX2C(j+1,col,nb_lgn+1)] /= s;\n",
        "          }\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "/* Kernel to compute relu on matrix of batch,\n",
        "   taking one thread in the block for each element:\n",
        "      X = theta(Z)\n",
        "   Attention: \n",
        "   X has one row more than Z; the first row of X is identical \n",
        "   to 1 by convention!\n",
        "*/\n",
        "__global__ \n",
        "void relu_kernel(float* X, float* Z, int nb_lgn, int nb_col) {\n",
        "    const unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    const unsigned int col = i/nb_lgn;\n",
        "    const unsigned int lgn = i%nb_lgn;\n",
        "\n",
        "    if (col<nb_col && lgn<nb_lgn) {\n",
        "      X[IDX2C(lgn+1,col,nb_lgn+1)] = Z[IDX2C(lgn,col,nb_lgn)] < 0.0 ? 0.0 : Z[IDX2C(lgn,col,nb_lgn)];\n",
        "      if (lgn == 0) {\n",
        "          // Assign first row of X to 1.0 by convention\n",
        "          X[IDX2C(0,col,nb_lgn+1)] = 1.0;\n",
        "      }\n",
        "      // printf(\"relu %d,%d,%f,%f\\n\",lgn,col,Z[IDX2C(lgn,col,nb_lgn)],X[IDX2C(lgn+1,col,nb_lgn+1)]);\n",
        "    }\n",
        "}\n",
        "\n",
        "/* Kernel to compute derivative of relu on matrix of batch,\n",
        "   taking one thread in the block for each element:\n",
        "      Q = d/dz theta(Z)\n",
        "*/\n",
        "__global__ \n",
        "void deriv_relu_kernel(float* Q, float* Z, int nb_lgn, int nb_col) {\n",
        "    const unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    const unsigned int col = i/nb_lgn;\n",
        "    const unsigned int lgn = i%nb_lgn;\n",
        "\n",
        "    if (col<nb_col && lgn<nb_lgn) {\n",
        "      Q[IDX2C(lgn,col,nb_lgn)] = Z[IDX2C(lgn,col,nb_lgn)] < 0.0 ? 0.0 : 1.0;\n",
        "      // printf(\"deriv relu %d,%d,%f,%f\\n\",lgn,col,Z[IDX2C(lgn,col,nb_lgn)],Q[IDX2C(lgn,col,nb_lgn)]);\n",
        "    }\n",
        "}\n",
        "\n",
        "/*  Forward Computation by one step with batch of size B,\n",
        "    assigning Z(l) and X(l)\n",
        "    Compute Z(l) = W(l)^T X(l-1)\n",
        "            X(l) = theta(Z(l))\n",
        "*/\n",
        "void forward_one(cublasHandle_t handle, int l, int* Ml, int B, float** W, float** X, float** Z, int* T) {\n",
        "    // W^T(l) has dimensions M(l) by M(l-1)+1\n",
        "    // X(l) has dimensions M(l)+1 by B\n",
        "    // Z(l) has dimensions M(l) by B\n",
        "    float alpha = 1.0;\n",
        "    float beta = 0.0;\n",
        "    int D = Ml[l-1]+1;\n",
        "    int M = Ml[l];\n",
        "    cublasStatus_t status = cublasSgemm(handle, CUBLAS_OP_T, CUBLAS_OP_N, \n",
        "                          M, B, D, &alpha, \n",
        "                          W[l], D, X[l-1], D, &beta, Z[l], M);\n",
        "    //printf(\"W[l]:\\n\");print_GPU_array(W[l],D,M);\n",
        "    //printf(\"X[l-1]:\\n\");print_GPU_array(X[l-1],D,B);\n",
        "    //printf(\"Z[l]:\\n\");print_GPU_array(Z[l],M,B);\n",
        "    // loop over elements of Z to compute theta(Z)\n",
        "    if (T[l]==0) { \n",
        "      // softmax\n",
        "      softmax_kernel<<<(M*B-1)/1024+1, 1024>>>(X[l],Z[l],M,B);\n",
        "    } else { // relu\n",
        "      // relu\n",
        "      relu_kernel<<<(M*B-1)/1024+1, 1024>>>(X[l],Z[l],M,B);     \n",
        "    }\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "    gpuErrchk( cudaDeviceSynchronize() );\n",
        "    //printf(\"X[l]:\\n\");print_GPU_array(X[l],M+1,B);\n",
        "}\n",
        "\n",
        "/* Run the forward computation through the network,\n",
        "   starting from a batch X0.\n",
        "   The output is X[L-1]. \n",
        "   \n",
        "   Note: We assume all arrays are allocated. \n",
        "*/\n",
        "void forward_comp(cublasHandle_t handle, int L, int* Ml, int B, \n",
        "                  float** W, float** X, float** Z, \n",
        "                  int* T, float* X0) {\n",
        "    // set X(0) = X0\n",
        "    X[0] = X0;\n",
        "    // from l = 1 to L, compute X(l) and Z(l)\n",
        "    for (int l = 1; l<L; ++l) {\n",
        "        forward_one(handle,l,Ml,B,W,X,Z,T);\n",
        "    }\n",
        "}\n",
        "\n",
        "/* Computes E = X-Y.T,\n",
        "   where X is taken only from the 2nd row.\n",
        "\n",
        "   E is M by B; \n",
        "   X is M+1 by B;\n",
        "   Y is M by B\n",
        "*/\n",
        "__global__\n",
        "void back_init_kernel(float* E, const float* X, const float* Y, int M, int B) {\n",
        "    const unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    const unsigned int col = i/M;\n",
        "    const unsigned int lgn = i%M;\n",
        "\n",
        "    E[IDX2C(lgn,col,M)] = X[IDX2C(lgn+1,col,M+1)] - Y[IDX2C(lgn,col,M)];\n",
        "}\n",
        "\n",
        "/* Computes the element-wise product of \n",
        "      A and theta'(Z),\n",
        "   leaving out the first row of A,\n",
        "   and assigns it to E.\n",
        "   E is M by B, A is M+1 by B, Z is M by B.\n",
        "*/\n",
        "__global__\n",
        "void back_combine_kernel_relu(float* E, const float* A, const float* Z, int M, int B) {\n",
        "    const unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    const unsigned int col = i/M;\n",
        "    const unsigned int lgn = i%M;\n",
        "\n",
        "    E[IDX2C(lgn,col,M)] = Z[IDX2C(lgn,col,M)] < 0 ? 0.0 : A[IDX2C(lgn+1,col,M+1)];\n",
        "}\n",
        "\n",
        "/* Run the backward propagation through the network,\n",
        "   starting from a batch of outputs Y.\n",
        "   Y is M by B, containing one output per column.\n",
        "   Note: We assume all arrays are allocated. \n",
        "*/\n",
        "void back_prop(cublasHandle_t handle, int L, int* Ml, int B, \n",
        "               float** W, \n",
        "               float** X, float** Z, \n",
        "               float** A, float** E,\n",
        "               int* T, const float* Y) {\n",
        "    int l = L-1;\n",
        "    // initialize with\n",
        "    //    E(l) = X(l)-Y\n",
        "    // first copy X to E, without the first row of X\n",
        "    int nb = Ml[l]*B;\n",
        "    back_init_kernel<<<(nb-1)/1024+1, 1024>>>(E[l],X[l],Y,Ml[l],B);\n",
        "\n",
        "    // from l = L to 1, compute \n",
        "    //    A(l) = W(l) E(l)   // the first row here comes from the bias term\n",
        "    //    E(l-1) = combine( A(l) , Z(l-1) )\n",
        "    for (; l>1; --l) {\n",
        "        float alpha = 1.0;\n",
        "        float beta = 0.0;\n",
        "        int D = Ml[l-1]+1;\n",
        "        int M = Ml[l];\n",
        "        cublasStatus_t status = cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, \n",
        "                              D, B, M, &alpha, \n",
        "                              W[l], D, E[l], M, &beta, A[l], D);\n",
        "        back_combine_kernel_relu<<<(nb-1)/1024+1, 1024>>>(E[l-1],A[l],Z[l-1],Ml[l-1],B);\n",
        "    }\n",
        "}\n",
        "\n",
        "/* Run one step of the gradient descent backwards through the\n",
        "   network, using the update\n",
        "      W[l] = W[l] - r/B X[l-1] E[l]^T,\n",
        "   where r is the learning rate.\n",
        "\n",
        "   W[l] is M[l-1]+1 by M[l]\n",
        "   X[l-1] is M[l-1]+1 by B\n",
        "   E[l] is M[l] by B\n",
        "*/\n",
        "void gradient_step(cublasHandle_t handle, int L, int* Ml, int B, \n",
        "               float** W, \n",
        "               float** X, float** E, float r) {\n",
        "    int l = L-1;\n",
        "    for (; l>=1; --l) {\n",
        "        float alpha = -r/(float)B;\n",
        "        float beta = 1.0;\n",
        "        int D = Ml[l-1]+1;\n",
        "        int M = Ml[l];\n",
        "        cublasStatus_t status = cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_T, \n",
        "                              D, M, B, &alpha, \n",
        "                              X[l-1], D, E[l], M, &beta, W[l], D);\n",
        "    }\n",
        "}\n",
        "\n",
        "/* Allocate memory for the network,\n",
        "   given L (number of layers),\n",
        "         M (size of each layer),\n",
        "         B (batch size).\n",
        "   This assigns addresses to the elements of the\n",
        "   (already existing arrays) W,X,Z. */\n",
        "void allocate_network_memory(int L, int* M, int B, \n",
        "                             float** W, \n",
        "                             float** X, float** Z,\n",
        "                             float** A, float** E) {\n",
        "    // go through each layer and reserve the appropriate size\n",
        "    // for l=0, we need only X\n",
        "    printf(\"allocating %d for X0\\n\",(M[0]+1) * B * sizeof(float));\n",
        "    cudaMalloc((void **)&X[0], (M[0]+1) * B * sizeof(float));\n",
        "\n",
        "    for (int l = 1; l < L; ++l) {\n",
        "      // W(l) has dimensions M(l-1)+1 by M(l) \n",
        "      // X(l),A(l) have dimensions M(l)+1 by B\n",
        "      // Z(l),E(l) have dimensions M(l) by B\n",
        "      printf(\"allocating %d b for W(%d)\\n\",(M[l-1]+1) * M[l] * sizeof(float),l);\n",
        "      printf(\"allocating %d b for X(%d)\\n\",(M[l]+1) * B * sizeof(float),l);\n",
        "      printf(\"allocating %d b for Z(%d)\\n\",M[l] * B * sizeof(float),l);\n",
        "      cudaMalloc((void **)&W[l], (M[l-1]+1) * M[l] * sizeof(float));\n",
        "      cudaMalloc((void **)&X[l], (M[l]+1) * B * sizeof(float));\n",
        "      cudaMalloc((void **)&Z[l], M[l] * B * sizeof(float));\n",
        "      cudaMalloc((void **)&A[l], (M[l]+1) * B * sizeof(float));\n",
        "      cudaMalloc((void **)&E[l], M[l] * B * sizeof(float));\n",
        "    }\n",
        "}\n",
        "\n",
        "/* Main */\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    \n",
        "    float *alldata = 0;\n",
        "    size_t N = 5000; // number of data points (Google: 17000)\n",
        "    size_t N_train = 2000; // points for training (Google: 12000)\n",
        "    size_t N_test = N-N_train; // points for validation\n",
        "    read_housing_csv(&alldata,N);\n",
        "    float* X = 0; \n",
        "    float* Y = 0;\n",
        "    size_t D; // number of features\n",
        "    size_t M; // number of labels\n",
        "    get_inputs_and_labels(alldata,&X,&Y,N,data_columns,&D,&M);\n",
        "    /* Inputs and labels are now available in X and Y.\n",
        "       Each input is a column in X; X is D x N\n",
        "       each label is a row in Y; Y is N x M\n",
        "    */\n",
        "    \n",
        "    int nb_iter = 200;\n",
        "    int periods = nb_iter;\n",
        "    float step_size = 1.0*1e-10;\n",
        "    \n",
        "    \n",
        "    cublasStatus_t status;\n",
        "    float *h_Z;\n",
        "    float *h_q;\n",
        "    float *d_X = 0;\n",
        "    float *d_W = 0;\n",
        "    float *d_Z = 0;\n",
        "    float *d_q = 0;\n",
        "    float *d_YT = 0;\n",
        "    float alpha = 1.0f;\n",
        "    float beta = 0.0f;\n",
        "    size_t nX = D * N; // nb of elements in X\n",
        "    size_t nW = D * M; // nb of elements in W\n",
        "    size_t nZ = M * N; // nb of elements in Z=W^TX\n",
        "    int i,j,k;\n",
        "    float logloss;\n",
        "    cublasHandle_t handle;\n",
        "\n",
        "    status = cublasCreate(&handle);\n",
        "\n",
        "    /* Allocate host memory for the matrices */\n",
        "    h_Z = (float *)malloc(nZ * sizeof(h_Z[0]));\n",
        "    h_q = (float *)malloc(M * sizeof(h_q[0]));\n",
        "\n",
        "    float* YT = 0; // transpose of Y\n",
        "    YT = (float *)malloc(N * M * sizeof(YT[0]));\n",
        "    // assign to YT the transpose of Y\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "      for (int j = 0; j < M; ++j) {\n",
        "          YT[IDX2C(j,i,M)] = Y[IDX2C(i,j,N)];\n",
        "      }\n",
        "    }\n",
        "\n",
        "    /* Initialize Weight Matrix \n",
        "       its dimension is nb_outputs * nb_inputs+1\n",
        "    */\n",
        "    float *h_W;\n",
        "    h_W = (float *)malloc(nW * sizeof(h_W[0]));\n",
        "    for (i = 0; i < nW  ; ++i) {\n",
        "        h_W[i] = 2e-3 * 1/sqrt(N) * float_rand( -1, 1 );\n",
        "    }\n",
        "\n",
        "    /* Allocate device memory for the matrices */\n",
        "    cudaMalloc((void **)&d_Z, nZ * sizeof(d_Z[0]));\n",
        "    cudaMalloc((void **)&d_X, nX * sizeof(d_X[0]));\n",
        "    cudaMalloc((void **)&d_W, nW * sizeof(d_W[0]));\n",
        "    cudaMalloc((void **)&d_q, M * sizeof(d_q[0]));\n",
        "    cudaMalloc((void **)&d_YT, N * M * sizeof(d_YT[0]));\n",
        "\n",
        "    /* Initialize the device matrices with the host matrices */\n",
        "    status = cublasSetVector(nX, sizeof(X[0]), X, 1, d_X, 1);\n",
        "    status = cublasSetVector(nW, sizeof(h_W[0]), h_W, 1, d_W, 1);\n",
        "    status = cublasSetVector(N*M, sizeof(YT[0]), YT, 1, d_YT, 1);\n",
        "\n",
        "\n",
        "    // structure of the network\n",
        "    int L = 2; // nb of layers\n",
        "    int Ml[L]; // array of size of each layer\n",
        "    Ml[0] = D-1;\n",
        "    Ml[1] = M; \n",
        "    int B = 512; // batch size\n",
        "    int T[L] = { -1, 0 }; // Activation type: none=-1, softmax=0\n",
        "    // create arrays for matrices\n",
        "    float* Wl[L];\n",
        "    float* Xl[L];\n",
        "    float* Zl[L];\n",
        "    float* Al[L];\n",
        "    float* El[L];\n",
        "\n",
        "    // allocate memory for the network\n",
        "    allocate_network_memory(L, Ml, B, Wl, Xl, Zl, Al, El);\n",
        "\n",
        "    // use existing weights from above\n",
        "    Wl[1] = d_W;\n",
        "\n",
        "    // reserve memory for getting the results back */\n",
        "    printf(\"allocating %d b for P\\n\",(Ml[L-1]+1)*B * sizeof(float));\n",
        "    float* P_out = (float*) malloc((Ml[L-1]+1)*B * sizeof(float));\n",
        "    float* Y_batch = (float*) malloc(Ml[L-1]*B * sizeof(float));\n",
        "\n",
        "    int batch_index = 0;\n",
        "\n",
        "    while (batch_index+B<=N) {\n",
        "        printf(\"treating batch at index %d\\n\",batch_index);\n",
        "\n",
        "      // forward computation, using existing weights Wl,\n",
        "      // computing Xl,Zl along the way\n",
        "      forward_comp(handle, L, Ml, B, Wl, Xl, Zl, T, &d_X[IDX2C(0,batch_index,D)]);\n",
        "\n",
        "      // backward propagation, using Wl, Xl, Zl,\n",
        "      // computing Al, El along the way\n",
        "      back_prop(handle, L, Ml, B, Wl, Xl, Zl, Al, El, T, &d_YT[IDX2C(0,batch_index,M)]);\n",
        "\n",
        "      // take a step in gradient descent, using Xl, El,\n",
        "      // modifying Wl\n",
        "      gradient_step(handle, L, Ml, B, Wl, Xl, El, step_size);\n",
        "      printf(\"W[1]:\\n\");print_GPU_array(Wl[1],D,M);\n",
        "\n",
        "      // get the result to the host\n",
        "      status = cublasGetVector((Ml[L-1]+1)*B, sizeof(float), Xl[L-1], 1, P_out, 1);\n",
        "\n",
        "      // take the argmax per element, ignoring the first row (which is 1.0 by convention)\n",
        "      int true_class = 0;\n",
        "      int nb_tested = 0;\n",
        "      for (int i=0;i<B;++i) { \n",
        "          // for each element in the batch, take the argmax\n",
        "          // over rows 1 to Ml[L-1]\n",
        "          float pmax = 0.0;\n",
        "          int fmax = 0;\n",
        "          for (int j=1;j<=Ml[L-1];++j) {\n",
        "              Y_batch[IDX2C(i,j-1,B)] = 0.0;\n",
        "              if (P_out[IDX2C(j,i,Ml[L-1]+1)] > pmax) {\n",
        "                pmax = P_out[IDX2C(j,i,Ml[L-1]+1)];\n",
        "                fmax = j-1;\n",
        "              }\n",
        "          }\n",
        "          Y_batch[IDX2C(i,fmax,B)] = 1.0;\n",
        "          // compare prediction to label\n",
        "          if (Y[IDX2C(i+batch_index,fmax,N)]>=0.5f) {\n",
        "              ++true_class;\n",
        "          }\n",
        "          ++nb_tested;\n",
        "      }\n",
        "\n",
        "      if (0) {\n",
        "        // print the result\n",
        "        for (int i=0;i<B;++i) { \n",
        "            for (int j=1;j<=Ml[L-1];++j) {\n",
        "                printf(\"p_%d=%f,y_%d=%f \",j-1,P_out[IDX2C(j,i,Ml[L-1]+1)],j-1,Y[IDX2C(i+N_train,j-1,N)]);\n",
        "            }\n",
        "            printf(\"\\n\");\n",
        "        }\n",
        "      }\n",
        "      \n",
        "      printf(\"Correct results: %d out of %d\\n\",true_class,nb_tested);\n",
        "      printf(\"Accuracy: %f\\n\",(float)true_class/(float)nb_tested);\n",
        "\n",
        "      batch_index += B;\n",
        "    }\n",
        "    printf(\"Finished.\\n\");\n",
        "\n",
        "    /**************************************************\n",
        "     * TO DO:\n",
        "     * write a function, to clean up the memory of the\n",
        "     * network\n",
        "     **************************************************\n",
        "\n",
        "    /* Memory clean up */\n",
        "    free(h_Z);\n",
        "    free(h_W);\n",
        "    free(h_q);\n",
        "\n",
        "    cudaFree(d_q);\n",
        "    cudaFree(d_X);\n",
        "    cudaFree(d_W);\n",
        "    cudaFree(d_Z);\n",
        "\n",
        "    /* Shutdown */\n",
        "    status = cublasDestroy(handle);\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "headers: \"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"!\n",
            "Read 5000 rows.\n",
            "Data (first 10 rows):\n",
            "-122.05\t37.37\t27\t3885\t661\t1537\t606\t6.6085\t344700\t\n",
            "-118.3\t34.26\t43\t1510\t310\t809\t277\t3.599\t176500\t\n",
            "-117.81\t33.78\t27\t3589\t507\t1484\t495\t5.7934\t270500\t\n",
            "-118.36\t33.82\t28\t67\t15\t49\t11\t6.1359\t330000\t\n",
            "-119.67\t36.33\t19\t1241\t244\t850\t237\t2.9375\t81700\t\n",
            "-119.56\t36.51\t37\t1018\t213\t663\t204\t1.6635\t67000\t\n",
            "-121.43\t38.63\t43\t1009\t225\t604\t218\t1.6641\t67000\t\n",
            "-120.65\t35.48\t19\t2310\t471\t1341\t441\t3.225\t166900\t\n",
            "-122.84\t38.4\t15\t3080\t617\t1446\t599\t3.6696\t194400\t\n",
            "-118.02\t34.08\t31\t2402\t632\t2830\t603\t2.3333\t164200\t\n",
            "Allocated memory: 5000 rows, 9 columns.\n",
            "Inputs (first 10):\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "-122.05\t-118.3\t-117.81\t-118.36\t-119.67\t-119.56\t-121.43\t-120.65\t-122.84\t-118.02\t\n",
            "37.37\t34.26\t33.78\t33.82\t36.33\t36.51\t38.63\t35.48\t38.4\t34.08\t\n",
            "27\t43\t27\t28\t19\t37\t43\t19\t15\t31\t\n",
            "3885\t1510\t3589\t67\t1241\t1018\t1009\t2310\t3080\t2402\t\n",
            "661\t310\t507\t15\t244\t213\t225\t471\t617\t632\t\n",
            "1537\t809\t1484\t49\t850\t663\t604\t1341\t1446\t2830\t\n",
            "606\t277\t495\t11\t237\t204\t218\t441\t599\t603\t\n",
            "6.6085\t3.599\t5.7934\t6.1359\t2.9375\t1.6635\t1.6641\t3.225\t3.6696\t2.3333\t\n",
            "Labels (first 10):\n",
            "1\t0\t\n",
            "0\t1\t\n",
            "1\t0\t\n",
            "1\t0\t\n",
            "0\t1\t\n",
            "0\t1\t\n",
            "0\t1\t\n",
            "0\t1\t\n",
            "0\t1\t\n",
            "0\t1\t\n",
            "allocating 18432 for X0\n",
            "allocating 72 b for W(1)\n",
            "allocating 6144 b for X(1)\n",
            "allocating 4096 b for Z(1)\n",
            "allocating 6144 b for P\n",
            "treating batch at index 0\n",
            "W[1]:\n",
            "0.000019 0.000003 \n",
            "-0.000006 -0.000001 \n",
            "0.000016 0.000007 \n",
            "0.000017 -0.000008 \n",
            "0.000023 0.000001 \n",
            "-0.000017 0.000026 \n",
            "-0.000009 0.000024 \n",
            "0.000015 0.000008 \n",
            "-0.000013 0.000012 \n",
            "Correct results: 363 out of 512\n",
            "Accuracy: 0.708984\n",
            "treating batch at index 512\n",
            "W[1]:\n",
            "0.000019 0.000003 \n",
            "-0.000006 -0.000001 \n",
            "0.000016 0.000007 \n",
            "0.000017 -0.000008 \n",
            "0.000023 0.000001 \n",
            "-0.000017 0.000026 \n",
            "-0.000009 0.000024 \n",
            "0.000015 0.000008 \n",
            "-0.000013 0.000012 \n",
            "Correct results: 347 out of 512\n",
            "Accuracy: 0.677734\n",
            "treating batch at index 1024\n",
            "W[1]:\n",
            "0.000019 0.000003 \n",
            "-0.000006 -0.000001 \n",
            "0.000016 0.000007 \n",
            "0.000017 -0.000008 \n",
            "0.000023 0.000001 \n",
            "-0.000017 0.000026 \n",
            "-0.000009 0.000024 \n",
            "0.000015 0.000008 \n",
            "-0.000013 0.000012 \n",
            "Correct results: 374 out of 512\n",
            "Accuracy: 0.730469\n",
            "treating batch at index 1536\n",
            "W[1]:\n",
            "0.000019 0.000003 \n",
            "-0.000006 -0.000001 \n",
            "0.000016 0.000007 \n",
            "0.000017 -0.000008 \n",
            "0.000023 0.000001 \n",
            "-0.000017 0.000026 \n",
            "-0.000009 0.000024 \n",
            "0.000015 0.000008 \n",
            "-0.000013 0.000012 \n",
            "Correct results: 361 out of 512\n",
            "Accuracy: 0.705078\n",
            "treating batch at index 2048\n",
            "W[1]:\n",
            "0.000019 0.000003 \n",
            "-0.000006 -0.000001 \n",
            "0.000016 0.000007 \n",
            "0.000017 -0.000008 \n",
            "0.000023 0.000001 \n",
            "-0.000017 0.000026 \n",
            "-0.000009 0.000024 \n",
            "0.000015 0.000008 \n",
            "-0.000013 0.000012 \n",
            "Correct results: 375 out of 512\n",
            "Accuracy: 0.732422\n",
            "treating batch at index 2560\n",
            "W[1]:\n",
            "0.000019 0.000003 \n",
            "-0.000006 -0.000001 \n",
            "0.000016 0.000007 \n",
            "0.000017 -0.000008 \n",
            "0.000023 0.000001 \n",
            "-0.000017 0.000026 \n",
            "-0.000010 0.000024 \n",
            "0.000015 0.000008 \n",
            "-0.000013 0.000012 \n",
            "Correct results: 329 out of 512\n",
            "Accuracy: 0.642578\n",
            "treating batch at index 3072\n",
            "W[1]:\n",
            "0.000019 0.000003 \n",
            "-0.000006 -0.000001 \n",
            "0.000016 0.000007 \n",
            "0.000017 -0.000008 \n",
            "0.000023 0.000001 \n",
            "-0.000017 0.000026 \n",
            "-0.000010 0.000024 \n",
            "0.000015 0.000008 \n",
            "-0.000013 0.000012 \n",
            "Correct results: 0 out of 512\n",
            "Accuracy: 0.000000\n",
            "treating batch at index 3584\n",
            "W[1]:\n",
            "0.000019 0.000003 \n",
            "-0.000006 -0.000001 \n",
            "0.000016 0.000007 \n",
            "0.000017 -0.000008 \n",
            "0.000023 0.000001 \n",
            "-0.000017 0.000026 \n",
            "-0.000010 0.000024 \n",
            "0.000015 0.000008 \n",
            "-0.000013 0.000012 \n",
            "Correct results: 0 out of 512\n",
            "Accuracy: 0.000000\n",
            "treating batch at index 4096\n",
            "W[1]:\n",
            "0.000019 0.000003 \n",
            "-0.000006 -0.000001 \n",
            "0.000016 0.000007 \n",
            "0.000017 -0.000008 \n",
            "0.000023 0.000001 \n",
            "-0.000017 0.000026 \n",
            "-0.000010 0.000024 \n",
            "0.000015 0.000008 \n",
            "-0.000013 0.000012 \n",
            "Correct results: 0 out of 512\n",
            "Accuracy: 0.000000\n",
            "Finished.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3sB8PNvck0r",
        "colab_type": "text"
      },
      "source": [
        "Experimental:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fGENmT1lc9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!shuf sample_data/california_housing_test.csv > sample_data/california_housing_test_shuffled.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSEH79AvnEF-",
        "colab_type": "code",
        "outputId": "505ad9af-db34-46f6-9074-ea4648dcb56f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!wc sample_data/california_housing_train.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  17001   17001 1706430 sample_data/california_housing_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZJl9GELnKiu",
        "colab_type": "code",
        "outputId": "0653d802-c3d9-41ab-e394-a4ddb4a28f22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!ls -la sample_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 55812\n",
            "drwxr-xr-x 1 root root     4096 Mar  6 10:15 .\n",
            "drwxr-xr-x 1 root root     4096 Mar  3 18:11 ..\n",
            "-rwxr-xr-x 1 root root     1697 Jan  1  2000 anscombe.json\n",
            "-rw-r--r-- 1 root root   301141 Mar  3 18:11 california_housing_test.csv\n",
            "-rw-r--r-- 1 root root   301141 Mar  6 10:15 california_housing_test_shuffled.csv\n",
            "-rw-r--r-- 1 root root  1706430 Mar  3 18:11 california_housing_train.csv\n",
            "-rw-r--r-- 1 root root 18289443 Mar  3 18:11 mnist_test.csv\n",
            "-rw-r--r-- 1 root root 36523880 Mar  3 18:11 mnist_train_small.csv\n",
            "-rwxr-xr-x 1 root root      930 Jan  1  2000 README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UkTfbUcchwg",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}